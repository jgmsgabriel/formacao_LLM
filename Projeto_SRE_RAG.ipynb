{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jgmsgabriel/formacao_LLM/blob/main/Projeto_SRE_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEG8KS746Wmu"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit langchain\n",
        "!pip install -q langchain_community langchain-huggingface langchain_ollama langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-hJoWdJEBmU"
      },
      "outputs": [],
      "source": [
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP19iHfeEETw"
      },
      "outputs": [],
      "source": [
        "import faiss\n",
        "from langchain_community.vectorstores import FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jf2iNG4hEGo-",
        "outputId": "f30a115a-e115-4520-b410-4b92a47e7068"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xo38vjjlEJb6",
        "outputId": "b0899bd1-7d8a-433e-a8f0-3f52e7263f00"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K‚†∏\u001b[1G\u001b[0K‚†º\u001b[1G\u001b[0K‚†¥\u001b[1G\u001b[0K‚†¶\u001b[1G\u001b[0K‚†ß\u001b[1G\u001b[0K‚†á\u001b[1G\u001b[0K‚†è\u001b[1G\u001b[0K‚†ã\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0K\n",
            "up to date, audited 23 packages in 2s\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K\n",
            "2 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerabilities\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n",
            "\u001b[1G\u001b[0K‚†π\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "!pip install -q python-dotenv\n",
        "!npm install -q localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uHxWcGDqEQZ0",
        "outputId": "e9a6f8f0-ea22-429d-95e4-ebd43f25cbe0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting .env\n"
          ]
        }
      ],
      "source": [
        "%%writefile .env\n",
        "HUGGINGFACE_API_KEY=XXXXXXXXXXXXXXXXXXXXXX\n",
        "HUGGINGFACEHUB_API_TOKEN=XXXXXXXXXXXXXXXXXXXXXX\n",
        "LANGCHAIN_API_KEY=XXXXXXXXXXXXXXXXXXXXXX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0P5CTavqvh74",
        "outputId": "a2ded83d-d624-4560-8dae-8bb4e7616801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting projeto3.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile projeto3.py\n",
        "\n",
        "import streamlit as st\n",
        "from langchain_core.messages import AIMessage, HumanMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
        "\n",
        "import torch\n",
        "from langchain_huggingface import ChatHuggingFace\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "import faiss\n",
        "import tempfile\n",
        "import os\n",
        "import time\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "st.set_page_config(page_title=\"Converse com documentos üìö\", page_icon=\"üìö\")\n",
        "st.title(\"Converse com documentos üìö\")\n",
        "\n",
        "model_class = \"hf_hub\"\n",
        "\n",
        "#def model_hf_hub(model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", temperature=0.1):\n",
        "def model_hf_hub(model=\"meta-llama/Meta-Llama-3-8B-Instruct\", temperature=0.1):\n",
        "  llm = HuggingFaceHub(\n",
        "      repo_id=model,\n",
        "      model_kwargs={\n",
        "          \"temperature\": temperature,\n",
        "          \"return_full_text\": False,\n",
        "          \"max_new_tokens\": 512,\n",
        "          #\"torch_dtype\": torch.bfloat16,\n",
        "          #\"stop\": [\"<|eot_id|>\"],\n",
        "      }\n",
        "  )\n",
        "  return llm\n",
        "\n",
        "\n",
        "\n",
        "def config_retriever(uploads):\n",
        "    # Carregar documentos\n",
        "    docs = []\n",
        "    temp_dir = tempfile.TemporaryDirectory()\n",
        "    for file in uploads:\n",
        "        temp_filepath = os.path.join(temp_dir.name, file.name)\n",
        "        with open(temp_filepath, \"wb\") as f:\n",
        "            f.write(file.getvalue())\n",
        "        loader = PyPDFLoader(temp_filepath)\n",
        "        docs.extend(loader.load())\n",
        "\n",
        "    # Divis√£o em peda√ßos de texto / Split\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "\n",
        "    # Embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    # Armazenamento\n",
        "    vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "    vectorstore.save_local('vectorstore/db_faiss')\n",
        "\n",
        "    # Configurando o recuperador de texto / Retriever\n",
        "    retriever = vectorstore.as_retriever(\n",
        "        search_type='mmr',\n",
        "        search_kwargs={'k':3, 'fetch_k':4}\n",
        "    )\n",
        "\n",
        "    return retriever\n",
        "\n",
        "\n",
        "def config_rag_chain(model_class, retriever):\n",
        "\n",
        "    ### Carregamento da LLM\n",
        "    llm = model_hf_hub()\n",
        "\n",
        "    # Para defini√ß√£o dos prompts\n",
        "    if model_class.startswith(\"hf\"):\n",
        "        token_s, token_e = \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\", \"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
        "    else:\n",
        "        token_s, token_e = \"\", \"\"\n",
        "\n",
        "    # Prompt de contextualiza√ß√£o\n",
        "    context_q_system_prompt = \"Given the following chat history and the follow-up question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.\"\n",
        "\n",
        "    context_q_system_prompt = token_s + context_q_system_prompt\n",
        "    context_q_user_prompt = \"Question: {input}\" + token_e\n",
        "    context_q_prompt = ChatPromptTemplate.from_messages(\n",
        "        [\n",
        "            (\"system\", context_q_system_prompt),\n",
        "            MessagesPlaceholder(\"chat_history\"),\n",
        "            (\"human\", context_q_user_prompt),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Chain para contextualiza√ß√£o\n",
        "    history_aware_retriever = create_history_aware_retriever(\n",
        "        llm=llm, retriever=retriever, prompt=context_q_prompt\n",
        "    )\n",
        "\n",
        "    # Prompt para perguntas e respostas (Q&A)\n",
        "    qa_prompt_template = \"\"\"Voc√™ √© um assistente virtual especializado em Engenharia de Confiabilidade de Sites (SRE) e seu objetivo √© orientar sobre as melhores pr√°ticas do mercado, avaliar o cen√°rio atual da empresa e propor melhorias.\n",
        "Use os seguintes peda√ßos de contexto recuperado para responder √†s perguntas e recomenda√ß√µes.\n",
        "Se n√£o souber a resposta ou n√£o houver informa√ß√µes suficientes no contexto, informe que n√£o sabe, mas sugira caminhos para investiga√ß√£o ou pesquisa.\n",
        "Mantenha as respostas concisas, claras e pr√°ticas, sempre em portugu√™s. \\n\\n\n",
        "    Pergunta: {input} \\n\n",
        "    Contexto: {context}\"\"\"\n",
        "\n",
        "    qa_prompt = PromptTemplate.from_template(token_s + qa_prompt_template + token_e)\n",
        "\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "    rag_chain = create_retrieval_chain(\n",
        "        history_aware_retriever,\n",
        "        qa_chain,\n",
        "    )\n",
        "\n",
        "    return rag_chain\n",
        "\n",
        "\n",
        "uploads = st.sidebar.file_uploader(\n",
        "    label=\"Enviar arquivos\", type=[\"pdf\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "if not uploads:\n",
        "    st.info(\"Por favor, envie algum arquivo para continuar!\")\n",
        "    st.stop()\n",
        "\n",
        "\n",
        "if \"chat_history\" not in st.session_state:\n",
        "    st.session_state.chat_history = [\n",
        "        AIMessage(content=\"Ol√°, sou o seu assistente especializado em SRE! Como posso ajudar voc√™?\"),\n",
        "    ]\n",
        "\n",
        "if \"docs_list\" not in st.session_state:\n",
        "    st.session_state.docs_list = None\n",
        "\n",
        "if \"retriever\" not in st.session_state:\n",
        "    st.session_state.retriever = None\n",
        "\n",
        "for message in st.session_state.chat_history:\n",
        "    if isinstance(message, AIMessage):\n",
        "        with st.chat_message(\"AI\"):\n",
        "            st.write(message.content)\n",
        "    elif isinstance(message, HumanMessage):\n",
        "        with st.chat_message(\"Human\"):\n",
        "            st.write(message.content)\n",
        "\n",
        "start = time.time()\n",
        "user_query = st.chat_input(\"Digite sua mensagem aqui...\")\n",
        "\n",
        "if user_query is not None and user_query != \"\" and uploads is not None:\n",
        "\n",
        "    st.session_state.chat_history.append(HumanMessage(content=user_query))\n",
        "\n",
        "    with st.chat_message(\"Human\"):\n",
        "        st.markdown(user_query)\n",
        "\n",
        "    with st.chat_message(\"AI\"):\n",
        "\n",
        "        if st.session_state.docs_list != uploads:\n",
        "            print(uploads)\n",
        "            st.session_state.docs_list = uploads\n",
        "            st.session_state.retriever = config_retriever(uploads)\n",
        "\n",
        "        rag_chain = config_rag_chain(model_class, st.session_state.retriever)\n",
        "\n",
        "        result = rag_chain.invoke({\"input\": user_query, \"chat_history\": st.session_state.chat_history})\n",
        "\n",
        "        resp = result['answer']\n",
        "        st.write(resp)\n",
        "\n",
        "        # mostrar a fonte\n",
        "        sources = result['context']\n",
        "        for idx, doc in enumerate(sources):\n",
        "            source = doc.metadata['source']\n",
        "            file = os.path.basename(source)\n",
        "            page = doc.metadata.get('page', 'P√°gina n√£o especificada')\n",
        "\n",
        "            ref = f\":link: Fonte {idx}: *{file} - p. {page}*\"\n",
        "            print(ref)\n",
        "            with st.popover(ref):\n",
        "                st.caption(doc.page_content)\n",
        "\n",
        "    st.session_state.chat_history.append(AIMessage(content=resp))\n",
        "\n",
        "end = time.time()\n",
        "print(\"Tempo: \", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KrnuMrbds4Na"
      },
      "outputs": [],
      "source": [
        "!streamlit run projeto3.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMjgsW8c1NvZ",
        "outputId": "92f0fb4f-da05-40af-c96a-4aa5d00e6904"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "34.16.220.224\n",
            "\u001b[1G\u001b[0K‚†ô\u001b[1G\u001b[0Kyour url is: https://clear-suits-swim.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com\n",
        "\n",
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}